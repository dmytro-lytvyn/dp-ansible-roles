---
- hosts: data_platform
  become: true
  roles:
    - {role: 'basic', tags: 'basic'}
    - {role: 'firewall', tags: 'firewall'}
    - {role: 'unattended-upgrades', tags: 'unattended-upgrades'}
  vars:
    # Basic
    basic_user_name: "username"
    basic_user_pass: "encrypted-password-see-ansible-user-module-docs"
    #basic_user_ssh_key: "~/.ssh/id_rsa"
    #basic_user_public_keys:
    #  - public_keys/user1.pub
    #  - public_keys/user2.pub
    # Firewall
    firewall_open_ip_subnets:
      - "123.123.123.123/32" # Sample subnet
    firewall_dynamic_ip_hostname: "something.freemyip.com" # If you want to allow access from a dynamic IP, otherwise leave undefined
    firewall_dynamic_ip_scipt_path: "/opt/ufw_allow_dynamic_ip.sh"
    firewall_dynamic_ip_config_path: "/opt/ufw_dynamic_ip.conf"
    firewall_dynamic_ip_log_path: "/var/log/ufw_allow_dynamic_ip.log"
    # Unattended Upgrades
    upgrades_notification_email: "username@gmail.com"
    upgrades_notification_only_on_error: "false"
    upgrades_remove_unused_kernel_packages: "true"
    upgrades_remove_unused_dependencies: "true"
    upgrades_automatic_reboot: "true"
    upgrades_automatic_reboot_time: "02:00"


- hosts: rogue-1
  become: true
  roles:
    - {role: 'letsencrypt-certbot', tags: 'letsencrypt-certbot'}
  vars:
    # Configuration
    letsencrypt_domain_names: # If you use freemyip.com for domain names, put them here
      - "input.data-platform.example"
      - "schema-registry.data-platform.example"
    letsencrypt_notifications_email: "username@gmail.com"
    letsencrypt_truststore_file_name: "truststore.p12"
    letsencrypt_truststore_passwd: "Password_123"
    letsencrypt_renew_log_path: "/var/log/letsencrypt_renew.log"


- hosts: rogue-1
  become: true
  roles:
    - {role: 'java', tags: 'java'}
    - {role: 'nifi', tags: 'nifi'}
  vars:
    # Installation
    nifi_package_mirror: "https://archive.apache.org/dist"
    nifi_version: "1.19.0"

    nifi_archive_filename: "nifi-{{ nifi_version }}-bin.zip"
    nifi_archive_source: "{{ nifi_package_mirror }}/nifi/{{ nifi_version }}/{{ nifi_archive_filename }}"
    nifi_archive_hash_sha256: "54be4c13fa81a3fe4860a728288b79fadc1e122904b19f1279a12017412ebd1a"

    nifi_os_user_name: "nifi"
    nifi_os_group_name: "{{ nifi_os_user_name }}"

    # Memory config
    nifi_jvm_heap_memory_initial: "512m"
    nifi_jvm_heap_memory_max: "2560m"

    # Host/Port config
    nifi_freemyip_domain_name: "something.freemyip.com" # Dynamic IP service, leave undefined if not needed
    nifi_freemyip_domain_token: "something-token" # Dynamic IP service, leave undefined if not needed
    nifi_domain_name: "input.data-platform.example" # If you use freemyip.com for a domain name, put it here
    nifi_listener_port: 8443
    nifi_remote_input_port: 10443
    nifi_cluster_node_protocol_port: 11443

    # HTTPS config: we'll use the same truststore (storage of trusted authority certificates) as keystore (client key certificates) because we don't use them for authorization
    nifi_security_truststore_file_name: "truststore.p12"
    nifi_security_truststore_passwd: "Password_123"

    # OAuth config
    # https://console.developers.google.com/ -> New Project -> APIs and Services -> Credentials
    # Create OAuth client ID -> Web application, Authorized redirect URIs: https://{{ nifi_domain_name }}:{{ nifi_listener_port }}/nifi-api/access/oidc/callback
    nifi_oauth_admin_name: "username@gmail.com"
    nifi_oauth_client_id: "1234567890.apps.googleusercontent.com"
    nifi_oauth_client_secret: "OAuthSuperSecret"

    # API open ports
    firewall_open_ports: # If you need to expose an external API, leave undefined if not needed
      - '1337'


- hosts: rogue-1
  become: true
  roles:
    - {role: 'confluent-platform', tags: 'confluent-platform'}
  vars:
    # Installation versions
    confluent_os_user_name: "confluent"
    confluent_platform_version: "5.2"
    confluent_platform_scala_version: "2.12"

    # Installation components
    confluent_platform_install_kafka: true
    confluent_platform_install_connect_hdfs: true
    confluent_platform_install_schema_registry: true

    # Host/Port config
    schema_registry_freemyip_domain_name: "something-else.freemyip.com" # Dynamic IP service, leave undefined if not needed
    schema_registry_freemyip_domain_token: "something-else-token" # Dynamic IP service, leave undefined if not needed
    schema_registry_listener_port: 8081
    kafka_listener_domain_name: "input.data-platform.example" # If you use freemyip.com for a domain name, put it here

    # Environment config
    confluent_schema_registry_zk_port: 2181
    confluent_schema_registry_zk_namespace : "schema_registry"
    confluent_schema_registry_kafka_topic: "_schemas"


- hosts: rogue-1
  become: true
  roles:
    - {role: 'schema-registry-ui', tags: 'schema-registry-ui'}
    - {role: 'kafdrop', tags: 'kafdrop'}
  vars:
    # Installation
    schema_registry_ui_os_user_name: schema_registry_ui
    schema_registry_ui_version: "0.9.5"
    schema_registry_ui_archive_filename: "schema-registry-ui-{{ schema_registry_ui_version }}.tar.gz"
    schema_registry_ui_archive_source: "https://github.com/Landoop/schema-registry-ui/releases/download/v.{{ schema_registry_ui_version }}/schema-registry-ui-{{ schema_registry_ui_version }}.tar.gz"
    schema_registry_ui_archive_hash_sha256: "ed2e88d0e4d2c3b78ee389d2d1e2a7261b876a193651d1735a64bfdf4ba96a51"

    caddy_version: "0.11.0"
    caddy_archive_filename: "caddy_v{{ caddy_version }}_linux_amd64.tar.gz"
    caddy_archive_source: "https://github.com/mholt/caddy/releases/download/v{{ caddy_version }}/caddy_v{{ caddy_version }}_linux_amd64.tar.gz"
    caddy_archive_hash_sha256: "93e77bdbaba0a2b39f9e1de653d17ab1939491f7727948ec65750b4996d07c18"

    # Configuration
    schema_registry_domain_name: "schema-registry.data-platform.example" # If you use freemyip.com for a domain name, put it here
    schema_registry_listener_port : 8081
    caddy_listener_port: 8000

    # Kafdrop configuration
    kafdrop_base_url: "https://github.com/obsidiandynamics/kafdrop/releases/download"
    kafdrop_version: "1.19.0"

    kafdrop_jar_filename: "kafdrop-{{ kafdrop_version }}.jar"
    kafdrop_jar_url: "{{ kafdrop_base_url }}/{{ kafdrop_version }}/{{ kafdrop_jar_filename }}"

    kafdrop_os_user_name: "kafdrop"

    # Environment config
    kafdrop_zookeeper_host_name: "localhost"
    kafdrop_zookeeper_port: 2181

# All services status:

#for srv in nifi confluent schema kafdrop; do systemctl status $srv*.service --no-pager --full; done | grep -A 2 "●"


- hosts: rogue-2
  become: true
  roles:
    - {role: 'letsencrypt-certbot', tags: 'letsencrypt-certbot'}
  vars:
    # Configuration
    letsencrypt_domain_names: # If you use freemyip.com for domain names, put them here
      - "process.data-platform.example"
    letsencrypt_notifications_email: "username@gmail.com"
    letsencrypt_truststore_file_name: "truststore.p12"
    letsencrypt_truststore_passwd: "Password_123"
    letsencrypt_renew_log_path: "/var/log/letsencrypt_renew.log"


- hosts: rogue-2
  become: true
  roles:
    - {role: 'java', tags: 'java'}
    - {role: 'hadoop', tags: 'hadoop'}
    - {role: 'hive', tags: 'hive'}
    - {role: 'presto', tags: 'presto'}
  vars:
    # Hadoop
    hadoop_package_mirror: "http://www-eu.apache.org/dist"
    hadoop_version: "3.0.3"

    hadoop_archive_filename: "hadoop-{{ hadoop_version }}.tar.gz"
    hadoop_archive_url: "{{ hadoop_package_mirror }}/hadoop/common/hadoop-{{ hadoop_version }}/{{ hadoop_archive_filename }}"
    hadoop_archive_hash_sha256: "db96e2c0d0d5352d8984892dfac4e27c0e682d98a497b7e04ee97c3e2019277a"

    hadoop_unpack_dir: "/opt"
    hadoop_install_dir: "/opt/hadoop-{{ hadoop_version }}"
    hadoop_config_dir: "{{ hadoop_install_dir }}/etc/hadoop"
    hadoop_link_dir: "/opt/hadoop"
    hadoop_data_dir: /var/hadoop/data
    hadoop_dfs_namenode_name_dir: "{{ hadoop_data_dir }}/namenode"
    hadoop_dfs_datanode_data_dir: "{{ hadoop_data_dir }}/datanode"

    hadoop_os_user_name: "hdfs"
    hadoop_os_group_name: "{{ hadoop_os_user_name }}"
    hadoop_os_user_pass: "encrypted-password-see-ansible-user-module-docs"

    hadoop_host_name: "process.data-platform.example"
    hadoop_master_host_name: "{{ hadoop_host_name }}"
    hadoop_worker_host_names: "{{ hadoop_host_name }}"
    hadoop_dfs_replication: 1

    hadoop_install_master: true
    hadoop_install_worker: true
    hadoop_install_client: true

    # Hive
    hive_package_mirror: "http://www-eu.apache.org/dist"
    hive_version: "2.3.4"

    hive_archive_filename: "apache-hive-{{ hive_version }}-bin.tar.gz"
    hive_archive_url: "{{ hive_package_mirror }}/hive/hive-{{ hive_version }}/{{ hive_archive_filename }}"
    hive_archive_hash_sha256: "ce86d1c20b1004ef76b33feacf40aa7fc03b49de6299c424335fd7f6e875cea4"

    hive_unpack_dir: "/opt"
    hive_install_dir: "/opt/apache-hive-{{ hive_version }}-bin"
    hive_config_dir: "{{ hive_install_dir }}/conf"
    hive_link_dir: "/opt/hive"

    hive_metastore_host_name: "process.data-platform.example"
    hive_metastore_user_name: "hive"
    hive_metastore_user_pass: "Password_123"

    mysql_connector_version: "5.1.47"
    mysql_connector_filename: "mysql-connector-java-{{ mysql_connector_version }}.jar"
    mysql_connector_url: "http://central.maven.org/maven2/mysql/mysql-connector-java/{{ mysql_connector_version }}/{{ mysql_connector_filename }}"
    mysql_connector_hash_sha1: "9de4159aaf2d08817a276610b8114a825fca6cfd"
    mysql_connector_install_location: "{{ hive_install_dir }}/lib/mysql-connector-java.jar"

    hive_os_user_name: "hdfs"
    hive_os_group_name: "{{ hive_os_user_name }}"
    hive_os_user_pass: "$6$856oFray$yA7FyZVsnuaqouqF7ndPPgKmxkxUvT25x9rl.FQJBtvULe49CJ0jCx7u5qrBKvONLemMoyEVKuGTlz7FQkWYd1"

    hive_install_metastore: false

    # Presto
    presto_package_mirror: "https://repo1.maven.org/maven2/com/facebook/presto"
    presto_version: "0.213"

    presto_archive_filename: "presto-server-{{presto_version}}.tar.gz"
    presto_archive_url: "{{ presto_package_mirror }}/presto-server/{{ presto_version }}/{{ presto_archive_filename }}"
    presto_archive_hash_sha1: "fa389433287addaefe8eb926510ce74ef05814b9"

    presto_client_filename: "presto-cli-{{ presto_version }}-executable.jar"
    presto_client_url: "{{ presto_package_mirror }}/presto-cli/{{ presto_version }}/{{ presto_client_filename }}"
    presto_client_hash_sha1: "2fb5806d8889724e8df69444242a059d997f38b2"

    presto_unpack_dir: "/opt"
    presto_install_dir: "/opt/presto-server-{{ presto_version }}"
    presto_link_dir: "/opt/presto"
    presto_data_dir: /var/presto/data

    presto_os_user_name: "hdfs"
    presto_os_group_name: "{{ presto_os_user_name }}"
    presto_os_user_pass: "encrypted-password-see-ansible-user-module-docs"

    presto_coordinator_host: "process.data-platform.example"
    presto_hive_metastore_host: "{{ hive_metastore_host_name }}"
    presto_hive_allow_drop_table: true

    presto_install_coordinator: true
    presto_install_worker: true
    presto_install_client: true

    presto_query_max_memory: 2GB
    presto_query_max_memory_per_node: 2GB
    presto_query_max_total_memory_per_node: 2GB
    presto_max_heap_size: 3G
    presto_http_port: 9090

    presto_cluster_name: dataplatform
    presto_node_id: "{{presto_cluster_name}}-{{ presto_coordinator_host }}"

# All services status:

#for srv in hdfs yarn hive presto; do systemctl status $srv*.service --no-pager --full; done | grep -A 2 "●"


- hosts: rogue-2
  become: true
  roles:
    - {role: 'spark', tags: 'spark'}
  vars:
    # Spark
    spark_package_mirror: "http://www-eu.apache.org/dist"
    spark_version: "2.4.1"

    spark_archive_filename: "spark-{{ spark_version }}-bin-hadoop2.7.tgz"
    spark_archive_url: "{{ spark_package_mirror }}/spark/spark-{{ spark_version }}/{{ spark_archive_filename }}"
    spark_archive_hash_sha256: "f721123fea18a03c2b95fbe6f524212a2c1812d8fa9e6edf86468ea030854fd7"

    spark_unpack_dir: "/opt"
    spark_install_dir: "/opt/spark-{{ spark_version }}-bin-hadoop2.7"
    spark_link_dir: "/opt/spark"

    spark_os_user_name: "hdfs"
    spark_os_group_name: "{{ spark_os_user_name }}"
    spark_os_user_pass: "encrypted-password-see-ansible-user-module-docs"
